{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "# Set up plotting parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Dataset directory\n",
    "dataset_dir = '../dataset/'\n",
    "\n",
    "# GLOBAL CONFIGURATION: Set to True for weighted analysis, False for unweighted\n",
    "is_weighted = False\n",
    "file_prefix = 'weighted' if is_weighted else 'unweighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8151166",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Power Grid Network Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load all networks and basic statistics\n",
    "networks = {}\n",
    "network_stats = []\n",
    "\n",
    "print(\"Loading networks...\")\n",
    "for i in range(1, 59):\n",
    "    try:\n",
    "        data = loadmat(os.path.join(dataset_dir, f'{i}.mat'))\n",
    "        adj_matrix = data['A']\n",
    "        adj_np = adj_matrix.toarray()\n",
    "        assert (adj_np == adj_np.T).all(), \"Adjacency matrix is not symmetric\"\n",
    "        assert np.diag(adj_np).sum() == 0, \"Adjacency matrix has self-loops\"\n",
    "        \n",
    "        # Convert sparse matrix to NetworkX graph\n",
    "        if is_weighted and 'W' in data:\n",
    "            # Use distance matrix as weights\n",
    "            weight_matrix = data['W']\n",
    "            G = nx.from_scipy_sparse_array(weight_matrix)\n",
    "            print(f\"Network {i}: Using weighted graph with distance matrix\")\n",
    "        else:\n",
    "            # Use unweighted adjacency matrix\n",
    "            G = nx.from_scipy_sparse_array(adj_matrix)\n",
    "            if is_weighted:\n",
    "                print(f\"Network {i}: Warning - 'W' matrix not found, using unweighted\")\n",
    "        \n",
    "        # Remove self-loops and ensure undirected\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        G = G.to_undirected()\n",
    "        \n",
    "        networks[i] = G\n",
    "        \n",
    "        # Collect basic statistics\n",
    "        stats = {\n",
    "            'network_id': i,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'density': nx.density(G),\n",
    "            'is_connected': nx.is_connected(G)\n",
    "        }\n",
    "        network_stats.append(stats)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Loaded network {i}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading network {i}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(networks)} networks\")\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "stats_df = pd.DataFrame(network_stats)\n",
    "print(f\"\\nNetwork Statistics Summary:\")\n",
    "print(stats_df.describe())\n",
    "\n",
    "print(f\"\\nTop 5 largest networks by number of nodes:\")\n",
    "largest_networks = stats_df.nlargest(5, 'nodes')\n",
    "print(largest_networks[['network_id', 'nodes', 'edges']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 1: Average Nearest Neighbor Degree (Knn) Analysis\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 1: Average Nearest Neighbor Degree Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def compute_knn(G):\n",
    "    \"\"\"\n",
    "    Compute average nearest neighbor degree for each degree value\n",
    "    Returns dictionary {degree: avg_neighbor_degree}\n",
    "    \"\"\"\n",
    "    knn = nx.average_neighbor_degree(G)\n",
    "    degree_dict = dict(G.degree())\n",
    "    \n",
    "    # Group by degree and compute average knn for each degree\n",
    "    degree_knn = defaultdict(list)\n",
    "    for node, neighbor_deg in knn.items():\n",
    "        node_degree = degree_dict[node]\n",
    "        degree_knn[node_degree].append(neighbor_deg)\n",
    "    \n",
    "    # Average knn for each degree\n",
    "    avg_knn = {}\n",
    "    for degree, neighbor_degrees in degree_knn.items():\n",
    "        avg_knn[degree] = np.mean(neighbor_degrees)\n",
    "    \n",
    "    return avg_knn\n",
    "\n",
    "# Compute Knn for all networks\n",
    "all_knn_data = {}\n",
    "all_degrees = set()\n",
    "\n",
    "print(\"Computing Knn for all networks...\")\n",
    "for net_id, G in networks.items():\n",
    "    knn_dict = compute_knn(G)\n",
    "    all_knn_data[net_id] = knn_dict\n",
    "    all_degrees.update(knn_dict.keys())\n",
    "\n",
    "all_degrees = sorted(all_degrees)\n",
    "\n",
    "# Plot 1: All Knn curves together\n",
    "plt.figure(figsize=(14, 8))\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(networks)))\n",
    "\n",
    "for i, (net_id, knn_dict) in enumerate(all_knn_data.items()):\n",
    "    degrees = sorted(knn_dict.keys())\n",
    "    knn_values = [knn_dict[d] for d in degrees]\n",
    "    plt.plot(degrees, knn_values, alpha=0.9, color=colors[i], linewidth=1, \n",
    "             label=f'Grid {net_id}')\n",
    "\n",
    "plt.xlabel('Node Degree (k)')\n",
    "plt.ylabel('Average Nearest Neighbor Degree Knn(k)')\n",
    "plt.title('Average Nearest Neighbor Degree for All 58 Power Grids')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.grid(True, alpha=0.9)\n",
    "plt.savefig('../results/{}_knn_plot.pdf'.format(file_prefix), dpi=600)\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Average Knn across all networks\n",
    "print(\"Computing ensemble average...\")\n",
    "\n",
    "# Compute average Knn across all networks for each degree\n",
    "degree_knn_collections = defaultdict(list)\n",
    "for net_id, knn_dict in all_knn_data.items():\n",
    "    for degree, knn_val in knn_dict.items():\n",
    "        degree_knn_collections[degree].append(knn_val)\n",
    "\n",
    "# Calculate mean and std for each degree\n",
    "ensemble_degrees = []\n",
    "ensemble_mean = []\n",
    "ensemble_std = []\n",
    "\n",
    "for degree in sorted(degree_knn_collections.keys()):\n",
    "    if len(degree_knn_collections[degree]) >= 3:  # Only include degrees with at least 3 data points\n",
    "        ensemble_degrees.append(degree)\n",
    "        ensemble_mean.append(np.mean(degree_knn_collections[degree]))\n",
    "        ensemble_std.append(np.std(degree_knn_collections[degree]))\n",
    "\n",
    "ensemble_degrees = np.array(ensemble_degrees)\n",
    "ensemble_mean = np.array(ensemble_mean)\n",
    "ensemble_std = np.array(ensemble_std)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.errorbar(ensemble_degrees, ensemble_mean, yerr=ensemble_std, \n",
    "             fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=6,\n",
    "             color='red', ecolor='darkred', alpha=0.8)\n",
    "plt.fill_between(ensemble_degrees, ensemble_mean - ensemble_std, \n",
    "                 ensemble_mean + ensemble_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Node Degree (k)')\n",
    "plt.ylabel('Ensemble Average Knn(k)')\n",
    "plt.title('Ensemble Average of Nearest Neighbor Degree Across 58 Power Grids')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.grid(True, alpha=0.8)\n",
    "# plt.tight_layout()\n",
    "plt.savefig('../results/{}_knn_avg_plot.pdf'.format(file_prefix), dpi=600)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Infrastructure Ensemble Trends Discussion:\")\n",
    "print(f\"- Total degree range observed: {min(all_degrees)} to {max(all_degrees)}\")\n",
    "print(f\"- Number of networks contributing to ensemble: {len(networks)}\")\n",
    "print(f\"- Degree values with sufficient data (≥3 networks): {len(ensemble_degrees)}\")\n",
    "\n",
    "# Analyze trend\n",
    "if len(ensemble_degrees) > 5:\n",
    "    # Fit power law: Knn(k) ~ k^γ\n",
    "    log_k = np.log10(ensemble_degrees)\n",
    "    log_knn = np.log10(ensemble_mean)\n",
    "    \n",
    "    # Linear fit in log-log space\n",
    "    coeffs = np.polyfit(log_k, log_knn, 1)\n",
    "    gamma = coeffs[0]\n",
    "    \n",
    "    print(f\"- Power law exponent γ ≈ {gamma:.3f}\")\n",
    "    if gamma < -0.1:\n",
    "        print(\"- Networks show disassortative mixing (high-degree nodes connect to low-degree nodes)\")\n",
    "    elif gamma > 0.1:\n",
    "        print(\"- Networks show assortative mixing (high-degree nodes connect to high-degree nodes)\")\n",
    "    else:\n",
    "        print(\"- Networks show neutral mixing patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99974cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 2: Normalized Betweenness Centrality Analysis\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 2: Normalized Betweenness Centrality Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "all_betweenness = []\n",
    "\n",
    "print(\"Computing betweenness centrality for all networks...\")\n",
    "for net_id, G in networks.items():\n",
    "    # Compute normalized betweenness centrality\n",
    "    if is_weighted:\n",
    "        # For weighted graphs, use edge weights (distances) in shortest path calculation\n",
    "        bc = nx.betweenness_centrality(G, normalized=True, weight='weight')\n",
    "    else:\n",
    "        # For unweighted graphs\n",
    "        bc = nx.betweenness_centrality(G, normalized=True)\n",
    "    all_betweenness.extend(list(bc.values()))\n",
    "    \n",
    "    if net_id % 10 == 0:\n",
    "        print(f\"Processed network {net_id}\")\n",
    "\n",
    "all_betweenness = np.array(all_betweenness)\n",
    "\n",
    "print(f\"\\nBetweenness Centrality Statistics:\")\n",
    "print(f\"Total nodes across all networks: {len(all_betweenness)}\")\n",
    "print(f\"Mean betweenness centrality: {np.mean(all_betweenness):.6f}\")\n",
    "print(f\"Std betweenness centrality: {np.std(all_betweenness):.6f}\")\n",
    "print(f\"Max betweenness centrality: {np.max(all_betweenness):.6f}\")\n",
    "print(f\"Min betweenness centrality: {np.min(all_betweenness):.6f}\")\n",
    "\n",
    "# Plot histogram of betweenness centrality\n",
    "plt.figure(figsize=(10, 12))\n",
    "\n",
    "# Subplot 1: Linear scale histogram\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(all_betweenness, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Normalized Betweenness Centrality')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Betweenness Centrality (Linear Scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Log scale histogram (for better visualization of tail)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(all_betweenness[all_betweenness > 0], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Normalized Betweenness Centrality')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Betweenness Centrality (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# # Subplot 3: Cumulative distribution\n",
    "# plt.subplot(2, 2, 3)\n",
    "# sorted_bc = np.sort(all_betweenness)\n",
    "# cumulative = np.arange(1, len(sorted_bc) + 1) / len(sorted_bc)\n",
    "# plt.plot(sorted_bc, cumulative, linewidth=2, color='green')\n",
    "# plt.xlabel('Normalized Betweenness Centrality')\n",
    "# plt.ylabel('Cumulative Probability')\n",
    "# plt.title('Cumulative Distribution of Betweenness Centrality')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# # Subplot 4: Box plot by network size categories\n",
    "# plt.subplot(2, 2, 4)\n",
    "# # Categorize networks by size\n",
    "# size_categories = []\n",
    "# bc_by_size = []\n",
    "\n",
    "# small_nets = stats_df[stats_df['nodes'] <= 100]['network_id'].values\n",
    "# medium_nets = stats_df[(stats_df['nodes'] > 100) & (stats_df['nodes'] <= 500)]['network_id'].values\n",
    "# large_nets = stats_df[stats_df['nodes'] > 500]['network_id'].values\n",
    "\n",
    "# for net_id, G in networks.items():\n",
    "#     bc = list(nx.betweenness_centrality(G, normalized=True).values())\n",
    "#     if net_id in small_nets:\n",
    "#         size_categories.extend(['Small'] * len(bc))\n",
    "#         bc_by_size.extend(bc)\n",
    "#     elif net_id in medium_nets:\n",
    "#         size_categories.extend(['Medium'] * len(bc))\n",
    "#         bc_by_size.extend(bc)\n",
    "#     elif net_id in large_nets:\n",
    "#         size_categories.extend(['Large'] * len(bc))\n",
    "#         bc_by_size.extend(bc)\n",
    "\n",
    "# bc_df = pd.DataFrame({'Size': size_categories, 'Betweenness': bc_by_size})\n",
    "# sns.boxplot(data=bc_df, x='Size', y='Betweenness')\n",
    "# plt.title('Betweenness Centrality by Network Size')\n",
    "# plt.ylabel('Normalized Betweenness Centrality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/{}_betweenness_distribution.pdf'.format(file_prefix), dpi=600)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBetweenness Centrality Discussion:\")\n",
    "print(f\"- Most nodes have very low betweenness centrality (close to 0)\")\n",
    "print(f\"- Few nodes have high betweenness centrality, indicating they are critical bridges\")\n",
    "print(f\"- Distribution is highly right-skewed, typical of scale-free networks\")\n",
    "\n",
    "# Identify high-centrality nodes\n",
    "high_centrality_threshold = np.percentile(all_betweenness, 99)\n",
    "print(f\"- 99th percentile betweenness centrality: {high_centrality_threshold:.6f}\")\n",
    "print(f\"- {np.sum(all_betweenness > high_centrality_threshold)} nodes ({100*np.sum(all_betweenness > high_centrality_threshold)/len(all_betweenness):.2f}%) have very high centrality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d13d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 3: Closeness Centrality for Largest Networks and Correlation Analysis\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 3: Closeness Centrality Analysis for Largest Networks\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get the 5 largest networks\n",
    "largest_5_ids = largest_networks['network_id'].values\n",
    "print(f\"Analyzing closeness centrality for networks: {largest_5_ids}\")\n",
    "\n",
    "closeness_data = {}\n",
    "betweenness_data = {}\n",
    "correlation_results = []\n",
    "\n",
    "print(\"\\nComputing closeness and betweenness centralities...\")\n",
    "for net_id in largest_5_ids:\n",
    "    G = networks[net_id]\n",
    "    print(f\"Processing network {net_id} ({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)\")\n",
    "    \n",
    "    # Compute centralities\n",
    "    if is_weighted:\n",
    "        # For weighted graphs, use distance as weights\n",
    "        # Note: closeness centrality with weights uses shortest path distances\n",
    "        closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "        betweenness_centrality = nx.betweenness_centrality(G, normalized=True, weight='weight')\n",
    "        print(f\"  Using weighted centrality measures\")\n",
    "    else:\n",
    "        # For unweighted graphs\n",
    "        closeness_centrality = nx.closeness_centrality(G)\n",
    "        betweenness_centrality = nx.betweenness_centrality(G, normalized=True)\n",
    "        print(f\"  Using unweighted centrality measures\")\n",
    "    \n",
    "    # Store data\n",
    "    closeness_data[net_id] = list(closeness_centrality.values())\n",
    "    betweenness_data[net_id] = list(betweenness_centrality.values())\n",
    "    \n",
    "    # Compute correlation\n",
    "    cc_values = np.array(list(closeness_centrality.values()))\n",
    "    bc_values = np.array(list(betweenness_centrality.values()))\n",
    "    \n",
    "    correlation = np.corrcoef(cc_values, bc_values)[0, 1]\n",
    "    \n",
    "    correlation_results.append({\n",
    "        'network_id': net_id,\n",
    "        'nodes': G.number_of_nodes(),\n",
    "        'correlation': correlation\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "correlation_df = pd.DataFrame(correlation_results)\n",
    "print(f\"\\nCorrelation Results:\")\n",
    "print(correlation_df)\n",
    "\n",
    "# Plot comprehensive analysis\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# Plot 1 & 2: Distribution comparisons\n",
    "all_closeness = []\n",
    "all_betweenness_largest = []\n",
    "\n",
    "for net_id in largest_5_ids:\n",
    "    all_closeness.extend(closeness_data[net_id])\n",
    "    all_betweenness_largest.extend(betweenness_data[net_id])\n",
    "\n",
    "axes[0, 0].hist(all_closeness, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Closeness Centrality')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Closeness Centrality\\n(5 Largest Networks)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(all_betweenness_largest, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Betweenness Centrality')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Betweenness Centrality\\n(5 Largest Networks)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Correlation scatter plot (combined)\n",
    "axes[1, 0].scatter(all_closeness, all_betweenness_largest, alpha=0.6, s=20)\n",
    "axes[1, 0].set_xlabel('Closeness Centrality')\n",
    "axes[1, 0].set_ylabel('Betweenness Centrality')\n",
    "axes[1, 0].set_title('Closeness vs Betweenness Centrality\\n(All 5 Networks Combined)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient to plot\n",
    "overall_corr = np.corrcoef(all_closeness, all_betweenness_largest)[0, 1]\n",
    "axes[1, 0].text(0.05, 0.95, f'r = {overall_corr:.3f}', transform=axes[1, 0].transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                verticalalignment='top', fontsize=12)\n",
    "\n",
    "# Plot 4: Individual network correlations\n",
    "colors_net = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "for i, net_id in enumerate(largest_5_ids):\n",
    "    axes[1, 1].scatter(closeness_data[net_id], betweenness_data[net_id], \n",
    "                       alpha=0.6, s=15, color=colors_net[i], label=f'Net {net_id}')\n",
    "\n",
    "axes[1, 1].set_xlabel('Closeness Centrality')\n",
    "axes[1, 1].set_ylabel('Betweenness Centrality')\n",
    "axes[1, 1].set_title('Closeness vs Betweenness by Network')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Correlation coefficients bar chart\n",
    "axes[2, 0].bar(range(len(correlation_df)), correlation_df['correlation'], \n",
    "               color=['red', 'blue', 'green', 'purple', 'orange'])\n",
    "axes[2, 0].set_xlabel('Network ID')\n",
    "axes[2, 0].set_ylabel('Correlation Coefficient')\n",
    "axes[2, 0].set_title('Correlation between Closeness and Betweenness\\nby Network')\n",
    "axes[2, 0].set_xticks(range(len(correlation_df)))\n",
    "axes[2, 0].set_xticklabels([f'Net {id}' for id in correlation_df['network_id']])\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "axes[2, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Plot 6: Network size vs correlation\n",
    "axes[2, 1].axis(\"off\")\n",
    "# axes[2, 1].scatter(correlation_df['nodes'], correlation_df['correlation'], \n",
    "#                    s=100, color='darkred', alpha=0.7)\n",
    "# for i, row in correlation_df.iterrows():\n",
    "#     axes[2, 1].annotate(f'Net {row[\"network_id\"]}', \n",
    "#                        (row['nodes'], row['correlation']),\n",
    "#                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "# axes[2, 1].set_xlabel('Number of Nodes')\n",
    "# axes[2, 1].set_ylabel('Correlation Coefficient')\n",
    "# axes[2, 1].set_title('Network Size vs Centrality Correlation')\n",
    "# axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/{}_closeness_betweenness_analysis.pdf'.format(file_prefix), dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nCloseness Centrality Summary (5 largest networks):\")\n",
    "print(f\"Mean closeness centrality: {np.mean(all_closeness):.6f}\")\n",
    "print(f\"Std closeness centrality: {np.std(all_closeness):.6f}\")\n",
    "print(f\"Max closeness centrality: {np.max(all_closeness):.6f}\")\n",
    "print(f\"Min closeness centrality: {np.min(all_closeness):.6f}\")\n",
    "\n",
    "print(f\"\\nCorrelation Analysis Summary:\")\n",
    "print(f\"Overall correlation (all 5 networks): {overall_corr:.3f}\")\n",
    "print(f\"Mean correlation across networks: {correlation_df['correlation'].mean():.3f}\")\n",
    "print(f\"Std correlation across networks: {correlation_df['correlation'].std():.3f}\")\n",
    "\n",
    "print(f\"\\nCorrelation Discussion:\")\n",
    "if overall_corr > 0.7:\n",
    "    print(\"- Strong positive correlation between closeness and betweenness centrality\")\n",
    "elif overall_corr > 0.3:\n",
    "    print(\"- Moderate positive correlation between closeness and betweenness centrality\")\n",
    "elif overall_corr > 0:\n",
    "    print(\"- Weak positive correlation between closeness and betweenness centrality\")\n",
    "else:\n",
    "    print(\"- Little to no correlation between closeness and betweenness centrality\")\n",
    "\n",
    "print(\"- Nodes with high closeness centrality tend to be close to all other nodes\")\n",
    "print(\"- Nodes with high betweenness centrality lie on many shortest paths\")\n",
    "print(\"- In power grids, both measures often identify critical infrastructure nodes\")\n",
    "\n",
    "print(f\"\\nAnalysis Complete!\")\n",
    "print(f\"Summary: Analyzed {len(networks)} power grid networks with comprehensive centrality analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629085a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceve592",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
